{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/unsw-nb15-eda/train.csv\n",
      "/kaggle/input/unsw-nb15-eda/features.csv\n",
      "/kaggle/input/unsw-nb15-eda/test.csv\n",
      "/kaggle/input/unsw-nb15/UNSW-NB15_2.csv\n",
      "/kaggle/input/unsw-nb15/NUSW-NB15_features.csv\n",
      "/kaggle/input/unsw-nb15/UNSW-NB15_1.csv\n",
      "/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\n",
      "/kaggle/input/unsw-nb15/UNSW-NB15_3.csv\n",
      "/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\n",
      "/kaggle/input/unsw-nb15/UNSW-NB15_LIST_EVENTS.csv\n",
      "/kaggle/input/unsw-nb15/UNSW-NB15_4.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if '.csv' in filename:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing train test\n",
      "Dropping attack_cat\n",
      "Dropping id\n",
      "Dropping attack_cat\n",
      "Dropping id\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\n",
    "test = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv')\n",
    "if train.shape[0]<100000:\n",
    "    print(\"Fixing train test\")\n",
    "    train, test = test, train\n",
    "\n",
    "drop_columns = ['attack_cat', 'id']\n",
    "for df in [train, test]:\n",
    "    for col in drop_columns:\n",
    "        if col in df.columns:\n",
    "            print('Dropping '+col)\n",
    "            df.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def detection_rate(y_true, y_pred):\n",
    "    CM = metrics.confusion_matrix(y_true, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    CM = metrics.confusion_matrix(y_true, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    return FP/(FP+TN)\n",
    "\n",
    "def false_alarm_rate(y_true, y_pred):\n",
    "    CM = metrics.confusion_matrix(y_true, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    return (FP+FN)/(TP+TN+FP+FN)\n",
    "\n",
    "def get_xy(df):\n",
    "    return pd.get_dummies(df.drop(['label'], axis=1)), df['label']\n",
    "\n",
    "def get_cat_columns(train):\n",
    "    categorical = []\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            categorical.append(col)\n",
    "    return categorical\n",
    "\n",
    "def label_encode(train, test):\n",
    "    for col in get_cat_columns(train):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))\n",
    "    return train, test\n",
    "\n",
    "def process_feature(df):\n",
    "    df.loc[~df['state'].isin(['FIN', 'INT', 'CON', 'REQ', 'RST']), 'state'] = 'others'\n",
    "    df.loc[~df['service'].isin(['-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3']), 'service'] = 'others'\n",
    "    df.loc[df['proto'].isin(['igmp', 'icmp', 'rtp']), 'proto'] = 'igmp_icmp_rtp'\n",
    "    df.loc[~df['proto'].isin(['tcp', 'udp', 'arp', 'ospf', 'igmp_icmp_rtp']), 'proto'] = 'others'\n",
    "    return df\n",
    "\n",
    "def get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=StandardScaler()):\n",
    "    x_train, y_train = train.drop(['label'], axis=1), train['label']\n",
    "    x_test, y_test = test.drop(['label'], axis=1), test['label']\n",
    "    \n",
    "    if feature_engineer:\n",
    "        x_train, x_test = process_feature(x_train), process_feature(x_test)\n",
    "    \n",
    "    categorical_columns = get_cat_columns(x_train)\n",
    "    non_categorical_columns = [x for x in x_train.columns if x not in categorical_columns]\n",
    "    if scaler is not None:\n",
    "        x_train[non_categorical_columns] = scaler.fit_transform(x_train[non_categorical_columns])\n",
    "        x_test[non_categorical_columns] = scaler.transform(x_test[non_categorical_columns])\n",
    "\n",
    "    if label_encoding:\n",
    "        x_train, x_test = label_encode(x_train, x_test)\n",
    "        features = x_train.columns\n",
    "    else:\n",
    "        x_train = pd.get_dummies(x_train)\n",
    "        x_test = pd.get_dummies(x_test)\n",
    "        print(\"Column mismatch {0}, {1}\".format(set(x_train.columns)- set(x_test.columns),  set(x_test.columns)- set(x_train.columns)))\n",
    "        features = list(set(x_train.columns) & set(x_test.columns))\n",
    "    print(f\"Number of features {len(features)}\")\n",
    "    x_train = x_train[features]\n",
    "    x_test = x_test[features]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def results(y_test, y_pred):\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    pre = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    print(f\"Acc {acc}, Precision {pre}, Recall {rec}, F1-score {f1}\")\n",
    "    \n",
    "    CM = metrics.confusion_matrix(y_test, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    \n",
    "    # detection rate or true positive rate\n",
    "    DR = TP*100/(TP+FN)\n",
    "    # false positive rate\n",
    "    FPR = FP*100/(FP+TN)\n",
    "    # false alarm rate \n",
    "    FAR = (FP+FN)*100/(TP+TN+FP+FN)\n",
    "    \n",
    "    print(\"DR {0}, FPR {1}, FAR {2}\".format(DR, FPR, FAR))\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(params, X, Y):\n",
    "    y_probs = []\n",
    "    y_vals = []\n",
    "\n",
    "    # for tr_idx, val_idx in tqdm(kf.split(X, Y), total=folds):\n",
    "    for tr_idx, val_idx in kf.split(X, Y):\n",
    "        x_train, y_train = X.iloc[tr_idx], Y[tr_idx]\n",
    "        x_val, y_val = X.iloc[val_idx], Y[val_idx]\n",
    "        clf = RandomForestClassifier(**params)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_prob = clf.predict_proba(x_val)[:, 1]\n",
    "        \n",
    "        y_probs.append(y_prob)\n",
    "        y_vals.append(y_val)\n",
    "        \n",
    "    acc, pre, rec, f1, far, fpr, dr, auc = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    folds = len(y_probs)\n",
    "    for i in range(folds):\n",
    "        y_prob, y_val = y_probs[i], y_vals[i]\n",
    "        y_pred = np.where(y_prob>=0.5, 1, 0)\n",
    "\n",
    "        acc += metrics.accuracy_score(y_val, y_pred)/folds\n",
    "        f1 += metrics.f1_score(y_val, y_pred)/folds\n",
    "        pre += metrics.precision_score(y_val, y_pred) /folds\n",
    "        rec += metrics.recall_score(y_val, y_pred) /folds\n",
    "        dr += detection_rate(y_val, y_pred) /folds\n",
    "        fpr += false_positive_rate(y_val, y_pred) /folds\n",
    "        far += false_alarm_rate(y_val, y_pred)/folds\n",
    "        auc += metrics.roc_auc_score(y_val, y_prob) /folds \n",
    "    \n",
    "    print(f\"Acc {acc}, Precision {pre}, Recall {rec}, F1-score {f1} \\nFAR {far}, FPR {fpr}, DR {dr} , AUC {auc}\")\n",
    "    \n",
    "def test_run(params, X, Y):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    results(y_test, y_pred)\n",
    "    \n",
    "    y_prob = clf.predict_proba(x_test)[:, 1]\n",
    "    print(\"Auc {0}\".format(metrics.roc_auc_score(y_test, y_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "seed = 1\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'random_state':1,\n",
    "    'class_weight': {0:2, 1:1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why overfitting will be bad\n",
    "Evaluating the model on the same data, that was used for training gives wrong info about the actual performance of the model. Here we can see the model achieve nearly 100% accuracy of train data. However, when ten-fold cross validation is used on train data it reduces to 96%. And on test data the model's performance falls drastically to 87.16%. This indicates the model needs to be generalized and effective measures need to be taken to reduce overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features 42\n",
      "Acc 0.9964012980421009, Precision 0.9970855735892669, Recall 0.9976286439698008, F1-score 0.9973570348527938\n",
      "DR 99.76286439698008, FPR 0.6214285714285714, FAR 0.3598701957899179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     56000\n",
      "           1       1.00      1.00      1.00    119341\n",
      "\n",
      "    accuracy                           1.00    175341\n",
      "   macro avg       1.00      1.00      1.00    175341\n",
      "weighted avg       1.00      1.00      1.00    175341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y, x_test, y_test = get_train_test(train, test, feature_engineer=False, label_encoding=True, scaler=None)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X,Y)\n",
    "y_pred = clf.predict(X)\n",
    "results(Y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.9608705325583738, Precision 0.9633644554079561, Recall 0.979772254591989, F1-score 0.9714973621567162 \n",
      "FAR 0.039129467441626056, FPR 0.07941071428571428, DR 0.979772254591989 , AUC 0.9939725918435274\n"
     ]
    }
   ],
   "source": [
    "cross_validation(params, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.8722853811397755, Precision 0.818469531492966, Recall 0.9869407923762463, F1-score 0.8948447422371117\n",
      "DR 98.69407923762463, FPR 26.818918918918918, FAR 12.771461886022445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.84     37000\n",
      "           1       0.82      0.99      0.89     45332\n",
      "\n",
      "    accuracy                           0.87     82332\n",
      "   macro avg       0.90      0.86      0.87     82332\n",
      "weighted avg       0.89      0.87      0.87     82332\n",
      "\n",
      "Auc 0.9797695870228298\n"
     ]
    }
   ],
   "source": [
    "test_run(params, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations of Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column mismatch set(), set()\n",
      "Number of features 53\n"
     ]
    }
   ],
   "source": [
    "# Drop Features with low importance\n",
    "drop_columns = ['response_body_len', 'is_sm_ips_ports', 'ct_flw_http_mthd', 'trans_depth', 'dwin', 'ct_ftp_cmd', 'is_ftp_login']\n",
    "for df in [train, test]:\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "X, Y, x_test, y_test = get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Preprocess | Train Acc | Test Acc|\n",
    "|:-----:|:------:|:------:|\n",
    "| LabelEncoded | 95.92 | 87.40 |\n",
    "| OneHotEncoded | 95.65 | 87.87 |\n",
    "| OneHotEncoded, FeatureEngineer| 95.66 | 88.03 |\n",
    "| OneHotEncoded, FeatureEngineer, MinMaxScaler| 95.80 | 87.49 |\n",
    "| OneHotEncoded, FeatureEngineer, RobustScaler| 95.67 | 87.92 |\n",
    "| OneHotEncoded, FeatureEngineer, StandardScaler| 95.67 | 87.89 |\n",
    "| OneHotEncoded, FeatureEngineer, FeatureSelection, StandardScaler| 95.72 | 87.81 |\n",
    "| OneHotEncoded, FeatureEngineer, FeatureSelection, RobustScaler| 95.70 | 88.03 |\n",
    "| OneHotEncoded, FeatureEngineer, FeatureSelection, MinMaxScaler| 95.77 | 87.73 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 0.9561939218566241, Precision 0.9567817183292109, Recall 0.9799063154850629, F1-score 0.9682036274888738 \n",
      "FAR 0.04380607814337594, FPR 0.09433928571428572, DR 0.9799063154850629 , AUC 0.989753764302109\n",
      "Time spent in 10-fold cross validation of train data  36.799360999999976\n",
      "Acc 0.8791356945051741, Precision 0.8328347538146037, Recall 0.9764846024883085, F1-score 0.8989571804270787\n",
      "DR 97.64846024883084, FPR 24.013513513513512, FAR 12.086430549482582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     37000\n",
      "           1       0.83      0.98      0.90     45332\n",
      "\n",
      "    accuracy                           0.88     82332\n",
      "   macro avg       0.90      0.87      0.87     82332\n",
      "weighted avg       0.89      0.88      0.88     82332\n",
      "\n",
      "Auc 0.9643720997755896\n",
      "Time spent in test run  4.73065600000001\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'random_state':1,\n",
    "    'class_weight': {0:2, 1:1}\n",
    "}\n",
    "start_time = time.clock()\n",
    "cross_validation(params, X, Y)\n",
    "print(\"Time spent in 10-fold cross validation of train data \", time.clock()-start_time)\n",
    "\n",
    "start_time = time.clock()\n",
    "test_run(params, X, Y)\n",
    "print(\"Time spent in test run \", time.clock()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column mismatch set(), set()\n",
      "Number of features 53\n"
     ]
    }
   ],
   "source": [
    "X, Y, x_test, y_test = get_train_test(train, test, feature_engineer=True, label_encoding=False, scaler=RobustScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <th>PreProcessing</th> <th style=\"text-align:center\">Parameters</th><th>Train Acc</th><th>Test Acc</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"8\">OneHot encoding, StandardScaler</td><td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.10</td><td>91.05</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 10, max_depth 10, max_features 20, class_weight {0:2, 1: 1}</td> <td>94.16</td> <td>91.61</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 10, max_depth 20, max_features 20, class_weight {0:2, 1: 1}</td> <td>95.64</td> <td>89.16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 30, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td> <td>94.19</td> <td>91.66</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>n_estimators 10</td> <td>95.66</td> <td>87.90</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 10, max_depth 20</td> <td>95.73</td> <td>86.91</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 10, max_features 20</td> <td>95.76</td> <td>87.57</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>n_estimators 50</td> <td>96.06</td> <td>87.31</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td rowspan=\"2\">OneHot encoding, MinMaxScaler</td><td>n_estimators 10, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>93.98</td><td>91.49</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.20</td><td>91.32</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"3\">OneHot encoding, RobustScaler</td><td>n_estimators 10, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>94.08</td><td>91.90</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>n_estimators 50, max_depth 10, max_features 30, class_weight {0:2, 1: 1}</td><td>94.21</td><td>91.68</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>n_estimators 20, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.21</td><td>91.36</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"1\">OneHot encoding</td><td>n_estimators 10, max_depth 10, max_features 10, class_weight {0:2, 1: 1}</td><td>94.20</td><td>91.02</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators 20 max_features 10\n",
      "Acc 0.9424207824472288, Precision 0.9756968538115552, Recall 0.9387888848799665, F1-score 0.9568838403022358 \n",
      "FAR 0.057579217552771074, FPR 0.04983928571428572, DR 0.9387888848799665 , AUC 0.9912013708126307\n",
      "Acc 0.9128649856677841, Precision 0.8870136719542375, Recall 0.9646166063707756, F1-score 0.9241889464229104\n",
      "DR 96.46166063707756, FPR 15.054054054054054, FAR 8.713501433221591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     37000\n",
      "           1       0.89      0.96      0.92     45332\n",
      "\n",
      "    accuracy                           0.91     82332\n",
      "   macro avg       0.92      0.91      0.91     82332\n",
      "weighted avg       0.92      0.91      0.91     82332\n",
      "\n",
      "Auc 0.9840462193641625\n",
      "\n",
      "n_estimators 20 max_features 30\n",
      "Acc 0.9414797613949374, Precision 0.9774694053117982, Recall 0.9355879821913979, F1-score 0.9560672117854316 \n",
      "FAR 0.05852023860506269, FPR 0.04596428571428572, DR 0.9355879821913979 , AUC 0.9915677271127765\n",
      "Acc 0.9153913423699169, Precision 0.8930137266953493, Recall 0.9615282802435365, F1-score 0.9260053961037581\n",
      "DR 96.15282802435365, FPR 14.113513513513514, FAR 8.460865763008307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     37000\n",
      "           1       0.89      0.96      0.93     45332\n",
      "\n",
      "    accuracy                           0.92     82332\n",
      "   macro avg       0.92      0.91      0.91     82332\n",
      "weighted avg       0.92      0.92      0.91     82332\n",
      "\n",
      "Auc 0.985431124961545\n",
      "\n",
      "n_estimators 50 max_features 10\n",
      "Acc 0.9423352333934683, Precision 0.9757667397821258, Recall 0.9385877696693635, F1-score 0.9568136088359686 \n",
      "FAR 0.05766476660653176, FPR 0.049678571428571426, DR 0.9385877696693635 , AUC 0.9913899878521726\n",
      "Acc 0.9135694505174173, Precision 0.887712036360685, Recall 0.9651019147621989, F1-score 0.9247907330684028\n",
      "DR 96.51019147621989, FPR 14.956756756756757, FAR 8.643054948258271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     37000\n",
      "           1       0.89      0.97      0.92     45332\n",
      "\n",
      "    accuracy                           0.91     82332\n",
      "   macro avg       0.92      0.91      0.91     82332\n",
      "weighted avg       0.92      0.91      0.91     82332\n",
      "\n",
      "Auc 0.9844144900326959\n",
      "\n",
      "n_estimators 50 max_features 30\n",
      "Acc 0.9422325708211402, Precision 0.9774943995618361, Recall 0.936694031950199, F1-score 0.9566572310974865 \n",
      "FAR 0.05776742917885977, FPR 0.04596428571428571, DR 0.936694031950199 , AUC 0.991683118437803\n",
      "Acc 0.9152941748044503, Precision 0.8916479477230957, Recall 0.9632048001411806, F1-score 0.9260461071875463\n",
      "DR 96.32048001411806, FPR 14.340540540540541, FAR 8.470582519554972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     37000\n",
      "           1       0.89      0.96      0.93     45332\n",
      "\n",
      "    accuracy                           0.92     82332\n",
      "   macro avg       0.92      0.91      0.91     82332\n",
      "weighted avg       0.92      0.92      0.91     82332\n",
      "\n",
      "Auc 0.9857432262514874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_estimators in [20, 50]:\n",
    "    for max_features in [10, 30]:\n",
    "        print(\"n_estimators {0} max_features {1}\".format(n_estimators, max_features))\n",
    "        params = {\n",
    "           'n_estimators': n_estimators,\n",
    "            'random_state':1,\n",
    "            'max_depth':10,\n",
    "            'max_features': max_features,\n",
    "            'class_weight': {0:2, 1:1}\n",
    "        }\n",
    "        cross_validation(params, X, Y)\n",
    "        test_run(params, X, Y)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
